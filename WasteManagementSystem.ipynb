{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "jgVJZVQjt9N_"
      },
      "outputs": [],
      "source": [
        "#import all the dependencies\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D, AveragePooling2D\n",
        "from keras.layers import Input, concatenate\n",
        "import os, time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()"
      ],
      "metadata": {
        "id": "HS2mrsztvwGm"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLASS_NAMES = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
        "\n",
        "# Preparing the dataset\n",
        "X_val, y_val = train_images[:5000], train_labels[:5000]\n",
        "X_train, y_train = train_images[5000:], train_labels[5000:]\n",
        "X_test, y_test = test_images[500:], test_labels[500:]\n",
        "\n",
        "# Building tensorflow datasets\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_ds_size = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "test_ds_size = tf.data.experimental.cardinality(test_ds).numpy()\n",
        "validation_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "validation_ds_size = tf.data.experimental.cardinality(validation_ds).numpy()\n",
        "\n",
        "print(\"Training data size:\", train_ds_size)\n",
        "print(\"Test data size:\", test_ds_size)\n",
        "print(\"Validation data size:\", validation_ds_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk0y6728v4gZ",
        "outputId": "b5324c9d-fb70-4a21-940a-6f96e48eb1c6"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data size: 45000\n",
            "Test data size: 9500\n",
            "Validation data size: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AlexNet Model Code\n",
        "def alexnet_process_images(image, label):\n",
        "    # Normalize images to have a mean of 0 and standard deviation of 1\n",
        "    image = tf.image.per_image_standardization(image)\n",
        "    # Resize images from 28x28 to 227x227\n",
        "    image = tf.image.resize(image, (227,227))\n",
        "    return image, label\n",
        "\n",
        "alexnet_train_ds = (train_ds.map(alexnet_process_images).shuffle(buffer_size=train_ds_size).batch(batch_size=32, drop_remainder=True))\n",
        "alexnet_test_ds = (test_ds.map(alexnet_process_images).shuffle(buffer_size=test_ds_size).batch(batch_size=32, drop_remainder=True))\n",
        "alexnet_validation_ds = (validation_ds.map(alexnet_process_images).shuffle(buffer_size=validation_ds_size).batch(batch_size=32, drop_remainder=True))\n",
        "\n",
        "print(\"AlexNet data size:\", alexnet_train_ds)\n",
        "print(\"AlexNet data size:\", alexnet_test_ds)\n",
        "print(\"AlexNet data size:\", alexnet_validation_ds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpg5o4t6x9v6",
        "outputId": "8e0a419c-1df9-4a2c-a3c4-1e5061851124"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AlexNet data size: <BatchDataset element_spec=(TensorSpec(shape=(32, 227, 227, 3), dtype=tf.float32, name=None), TensorSpec(shape=(32, 1), dtype=tf.uint8, name=None))>\n",
            "AlexNet data size: <BatchDataset element_spec=(TensorSpec(shape=(32, 227, 227, 3), dtype=tf.float32, name=None), TensorSpec(shape=(32, 1), dtype=tf.uint8, name=None))>\n",
            "AlexNet data size: <BatchDataset element_spec=(TensorSpec(shape=(32, 227, 227, 3), dtype=tf.float32, name=None), TensorSpec(shape=(32, 1), dtype=tf.uint8, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def AlexNet():\n",
        "    # Sequential\n",
        "    keras.Sequential()\n",
        "\n",
        "    # input layer \n",
        "    input_layer = Input(shape = (227, 227, 3))\n",
        "\n",
        "    # convolutional layer: filters = 96, kernel_size = (11,11), strides = 4\n",
        "    X = Conv2D(filters = 96, kernel_size = (11,11), strides = 4, padding = 'same', activation = 'relu')(input_layer)\n",
        "\t\n",
        "\t# BatchNormalization()\n",
        "    X = BatchNormalization()(X)\n",
        "\n",
        "    # max-pooling layer: pool_size = (3,3), strides = 2\n",
        "    X = MaxPooling2D(pool_size = (3,3), strides = 2)(X)\n",
        "\n",
        "    # convolutional layer: filters = 256, kernel_size = (5,5), strides = 1\n",
        "    X = Conv2D(filters = 256, kernel_size = (5,5), strides = 1, padding = 'same', activation = 'relu')(X)\n",
        "\t\n",
        "\t# BatchNormalization()\n",
        "    X = BatchNormalization()(X)\n",
        "\n",
        "    # max-pooling layer: pool_size = (3,3), strides = 2\n",
        "    X = MaxPooling2D(pool_size = (3,3), strides = 2)(X)\n",
        "\t\n",
        "\t# convolutional layer: filters = 384, kernel_size = (3,3), strides = 1\n",
        "    X = Conv2D(filters = 384, kernel_size = (3,3), strides = 1, padding = 'same', activation = 'relu')(X)\n",
        "\t\n",
        "\t# BatchNormalization()\n",
        "    X = BatchNormalization()(X)\n",
        "\t\n",
        "\t# convolutional layer: filters = 384, kernel_size = (3,3), strides = 1\n",
        "    X = Conv2D(filters = 384, kernel_size = (3,3), strides = 1, padding = 'same', activation = 'relu')(X)\n",
        "\t\n",
        "\t# BatchNormalization()\n",
        "    X = BatchNormalization()(X)\n",
        "\t\n",
        "\t# convolutional layer: filters = 256, kernel_size = (3,3), strides = 1\n",
        "    X = Conv2D(filters = 256, kernel_size = (3,3), strides = 1, padding = 'same', activation = 'relu')(X)\n",
        "\t\n",
        "\t# BatchNormalization()\n",
        "    X = BatchNormalization()(X)\n",
        "\n",
        "    # max-pooling layer: pool_size = (3,3), strides = 2\n",
        "    X = MaxPooling2D(pool_size = (3,3), strides = 2)(X)\n",
        "\n",
        "    # Extra network:\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(4096, activation = 'relu')(X)\n",
        "\n",
        "    # Dropoutlayer \n",
        "    X = Dropout(0.5)(X)\n",
        "\n",
        "    # output layer \n",
        "    X = Dense(1000, activation = 'softmax')(X)\n",
        "    \n",
        "    return X"
      ],
      "metadata": {
        "id": "Q8nLfWVcx_Ek"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GoogleNet Model Code\n",
        "def googlenet_process_images(image, label):\n",
        "    # Normalize images to have a mean of 0 and standard deviation of 1\n",
        "    image = tf.image.per_image_standardization(image)\n",
        "    # Resize images from 28x28 to 224x224\n",
        "    image = tf.image.resize(image, (224,224))\n",
        "    return image, label\n",
        "\n",
        "googlenet_train_ds = (train_ds.map(googlenet_process_images).shuffle(buffer_size=train_ds_size).batch(batch_size=32, drop_remainder=True))\n",
        "googlenet_test_ds = (test_ds.map(googlenet_process_images).shuffle(buffer_size=test_ds_size).batch(batch_size=32, drop_remainder=True))\n",
        "googlenet_validation_ds = (validation_ds.map(googlenet_process_images).shuffle(buffer_size=validation_ds_size).batch(batch_size=32, drop_remainder=True))\n",
        "\n",
        "print(\"GoogleNet data size:\", googlenet_train_ds)\n",
        "print(\"GoogleNet data size:\", googlenet_test_ds)\n",
        "print(\"GoogleNet data size:\", googlenet_validation_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVfPsWsOyKja",
        "outputId": "1275cc3b-0cb3-4fbf-bb39-f0a3b689b4a0"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GoogleNet data size: <BatchDataset element_spec=(TensorSpec(shape=(32, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(32, 1), dtype=tf.uint8, name=None))>\n",
            "GoogleNet data size: <BatchDataset element_spec=(TensorSpec(shape=(32, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(32, 1), dtype=tf.uint8, name=None))>\n",
            "GoogleNet data size: <BatchDataset element_spec=(TensorSpec(shape=(32, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(32, 1), dtype=tf.uint8, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Inception_block(input_layer, f1, f2_conv1, f2_conv3, f3_conv1, f3_conv5, f4):\n",
        "    # Input: \n",
        "    # - f1: number of filters of the 1x1 convolutional layer in the first path\n",
        "    # - f2_conv1, f2_conv3 are number of filters corresponding to the 1x1 and 3x3 convolutional layers in the second path\n",
        "    # - f3_conv1, f3_conv5 are the number of filters corresponding to the 1x1 and 5x5  convolutional layer in the third path\n",
        "    # - f4: number of filters of the 1x1 convolutional layer in the fourth path\n",
        "\n",
        "    # 1st path:\n",
        "    path1 = Conv2D(filters = f1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n",
        "\n",
        "    # 2nd path\n",
        "    path2 = Conv2D(filters = f2_conv1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n",
        "    path2 = Conv2D(filters = f2_conv3, kernel_size = (3,3), padding = 'same', activation = 'relu')(path2)\n",
        "\n",
        "    # 3rd path\n",
        "    path3 = Conv2D(filters = f3_conv1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n",
        "    path3 = Conv2D(filters = f3_conv5, kernel_size = (5,5), padding = 'same', activation = 'relu')(path3)\n",
        "\n",
        "    # 4th path\n",
        "    path4 = MaxPooling2D((3,3), strides= (1,1), padding = 'same')(input_layer)\n",
        "    path4 = Conv2D(filters = f4, kernel_size = (1,1), padding = 'same', activation = 'relu')(path4)\n",
        "\n",
        "    output_layer = concatenate([path1, path2, path3, path4], axis = 3)\n",
        "\n",
        "    return output_layer"
      ],
      "metadata": {
        "id": "AfZcuqEHywkD"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GoogLeNet():\n",
        "    # input layer \n",
        "    input_layer = Input(shape = (224, 224, 3))\n",
        "\n",
        "    # convolutional layer: filters = 64, kernel_size = (7,7), strides = 2\n",
        "    X = Conv2D(filters = 64, kernel_size = (7,7), strides = 2, padding = 'same', activation = 'relu')(input_layer)\n",
        "\n",
        "    # max-pooling layer: pool_size = (3,3), strides = 2\n",
        "    X = MaxPooling2D(pool_size = (3,3), strides = 2)(X)\n",
        "\n",
        "    # convolutional layer: filters = 64, strides = 1\n",
        "    X = Conv2D(filters = 64, kernel_size = (1,1), strides = 1, padding = 'same', activation = 'relu')(X)\n",
        "\n",
        "    # convolutional layer: filters = 192, kernel_size = (3,3)\n",
        "    X = Conv2D(filters = 192, kernel_size = (3,3), padding = 'same', activation = 'relu')(X)\n",
        "\n",
        "    # max-pooling layer: pool_size = (3,3), strides = 2\n",
        "    X = MaxPooling2D(pool_size= (3,3), strides = 2)(X)\n",
        "\n",
        "    # 1st Inception block\n",
        "    X = Inception_block(X, f1 = 64, f2_conv1 = 96, f2_conv3 = 128, f3_conv1 = 16, f3_conv5 = 32, f4 = 32)\n",
        "\n",
        "    # 2nd Inception block\n",
        "    X = Inception_block(X, f1 = 128, f2_conv1 = 128, f2_conv3 = 192, f3_conv1 = 32, f3_conv5 = 96, f4 = 64)\n",
        "\n",
        "    # max-pooling layer: pool_size = (3,3), strides = 2\n",
        "    X = MaxPooling2D(pool_size= (3,3), strides = 2)(X)\n",
        "\n",
        "    # 3rd Inception block\n",
        "    X = Inception_block(X, f1 = 192, f2_conv1 = 96, f2_conv3 = 208, f3_conv1 = 16, f3_conv5 = 48, f4 = 64)\n",
        "\n",
        "    # Extra network 1:\n",
        "    X1 = AveragePooling2D(pool_size = (5,5), strides = 3)(X)\n",
        "    X1 = Conv2D(filters = 128, kernel_size = (1,1), padding = 'same', activation = 'relu')(X1)\n",
        "    X1 = Flatten()(X1)\n",
        "    X1 = Dense(1024, activation = 'relu')(X1)\n",
        "    X1 = Dropout(0.7)(X1)\n",
        "    X1 = Dense(5, activation = 'softmax')(X1)\n",
        "\n",
        "    # 4th Inception block\n",
        "    X = Inception_block(X, f1 = 160, f2_conv1 = 112, f2_conv3 = 224, f3_conv1 = 24, f3_conv5 = 64, f4 = 64)\n",
        "\n",
        "    # 5th Inception block\n",
        "    X = Inception_block(X, f1 = 128, f2_conv1 = 128, f2_conv3 = 256, f3_conv1 = 24, f3_conv5 = 64, f4 = 64)\n",
        "\n",
        "    # 6th Inception block\n",
        "    X = Inception_block(X, f1 = 112, f2_conv1 = 144, f2_conv3 = 288, f3_conv1 = 32, f3_conv5 = 64, f4 = 64)\n",
        "\n",
        "    # Extra network 2:\n",
        "    X2 = AveragePooling2D(pool_size = (5,5), strides = 3)(X)\n",
        "    X2 = Conv2D(filters = 128, kernel_size = (1,1), padding = 'same', activation = 'relu')(X2)\n",
        "    X2 = Flatten()(X2)\n",
        "    X2 = Dense(1024, activation = 'relu')(X2)\n",
        "    X2 = Dropout(0.7)(X2)\n",
        "    X2 = Dense(1000, activation = 'softmax')(X2)\n",
        "\n",
        "    # 7th Inception block\n",
        "    X = Inception_block(X, f1 = 256, f2_conv1 = 160, f2_conv3 = 320, f3_conv1 = 32, f3_conv5 = 128, f4 = 128)\n",
        "\n",
        "    # max-pooling layer: pool_size = (3,3), strides = 2\n",
        "    X = MaxPooling2D(pool_size = (3,3), strides = 2)(X)\n",
        "\n",
        "    # 8th Inception block\n",
        "    X = Inception_block(X, f1 = 256, f2_conv1 = 160, f2_conv3 = 320, f3_conv1 = 32, f3_conv5 = 128, f4 = 128)\n",
        "\n",
        "    # 9th Inception block\n",
        "    X = Inception_block(X, f1 = 384, f2_conv1 = 192, f2_conv3 = 384, f3_conv1 = 48, f3_conv5 = 128, f4 = 128)\n",
        "\n",
        "    # Global Average pooling layer \n",
        "    X = GlobalAveragePooling2D(name = 'GAPL')(X)\n",
        "\n",
        "    # Dropoutlayer \n",
        "    X = Dropout(0.4)(X)\n",
        "\n",
        "    # output layer \n",
        "    X = Dense(1000, activation = 'softmax')(X)\n",
        "\n",
        "    return X"
      ],
      "metadata": {
        "id": "NWAPGnHwyzbl"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AlexNetInput = AlexNet()\n",
        "print(\"AlexNet Model:\", AlexNetInput)\n",
        "\n",
        "GoogleNetInput = GoogLeNet()\n",
        "print(\"GoogleNet Model:\", GoogleNetInput)\n",
        "\n",
        "ModelInputLayer = concatenate([AlexNetInput, GoogleNetInput], axis = 1)\n",
        "print(\"Input Model:\", ModelInputLayer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_bJQrvhy141",
        "outputId": "ec4c5321-f6ca-4ac7-8095-143878e30544"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AlexNet Model: KerasTensor(type_spec=TensorSpec(shape=(None, 1000), dtype=tf.float32, name=None), name='dense_36/Softmax:0', description=\"created by layer 'dense_36'\")\n",
            "GoogleNet Model: KerasTensor(type_spec=TensorSpec(shape=(None, 1000), dtype=tf.float32, name=None), name='dense_41/Softmax:0', description=\"created by layer 'dense_41'\")\n",
            "Input Model: KerasTensor(type_spec=TensorSpec(shape=(None, 2000), dtype=tf.float32, name=None), name='concatenate_59/concat:0', description=\"created by layer 'concatenate_59'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input layer \n",
        "input_layer = Input(ModelInputLayer)\n",
        "\t\n",
        "# output layer \n",
        "ModelOutputLayer = Dense(10, activation = 'softmax')(input_layer)\n",
        "\n",
        "# model\n",
        "model = Model(inputs = input_layer, outputs = [ModelOutputLayer], name = 'SVMModel')\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "5ZaIPiB01isC",
        "outputId": "de602b7a-cee4-4ed4-f392-b752938e8389"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-e818caf8bcdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# input layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minput_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModelInputLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# output layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mModelOutputLayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/keras_tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot iterate over a scalar.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    409\u001b[0m                 \u001b[0;34m\"Cannot iterate over a Tensor with unknown first dimension.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             )\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot iterate over a Tensor with unknown first dimension."
          ]
        }
      ]
    }
  ]
}